#+title: CKY Parsing With Independence Constraints
#+author: Joseph Irwin
#+OPTIONS: H:2 toc:nil _:{}
#+LATEX_CLASS: acl2015
#+LATEX_HEADER: \usepackage{forest}
#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LATEX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LaTeX_HEADER: \newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\bigl(#1\bigr)}}

# file:paper.pdf

#+BEGIN_LaTeX
\begin{abstract}
We propose a novel property of words in a sentence, derived from a
context-free derivation, and show how this property can be used to
reduce the computation done by the CKY algorithm. We demonstrate a
classifier which can be used to identify boundaries between
independent words in a sentence using only surface features, and show
that it can be used to speed up a CFG parser.
\end{abstract}
#+END_LaTeX

* Introduction

- CFG parsing is widely used
- CKY algorithm is common
- there is constant research into approximate and partial parsing
- there is research into using classifiers to constrain CKY parser
- we propose a novel type of constraint, demonstrate a classifier to
  create these constraints, and show a speedup in a CFG parser

* Independence Constraints

- we define a property called *independence* which holds between two
  words in a sentence given a context-free derivation (parse tree) for
  that sentence
- conceptually, if two words are *independent* then a CKY parser
  doesn't have to consider any spans over those words until the very
  last cell in the chart

** Classifying Independent Span Boundaries
- in order to be useful, we need to be able to identify which words
  are independent using information which doesn't require a parser
- we can create a binary classifier using only POS features which decides
  whether any two consecutive words in a sentence are independent or not
- in order to control the tradeoff between speed and accuracy, a
  classifier that outputs a probability is desirable
- from previous unpublished work, we found that transforming the POS
  sequence and using the transformed sequence to create additional
  features can improve this type of classifier, so we propose features
  created from several *POS levels*
- features: local and global
  - local: left 1,2,3 tags, right 1,2,3 tags
  - global: left 1-,2-,3-grams, right 1-,2-,3-grams for POS levels
  - POS levels: level 0, 1, 2, 3
  
** Parsing With Independence Constraints
- alterations to the CKY algorithm
  - check constraint inside inner loop; or
  - loop between constraints first, then fill in cells which cross constraints
  - the difference is (likely) small compared to the work inside the
    loop(s)
- expected work saved (we can consider two ideal scenarios)
  - with one constraint at N/K
  - K constraints N/K words apart

* Classifying Independent Span Boundaries
- training and testing the classifier
  - we use opal in PA mode to train classifier and output probability
  - train on wsj02-21, test on wsj22
- results
- which features are useful:
  - local, global, local+global, etc

* Parsing With Independence Constraints
- modified the Stanford Parser CKY parser to accept independent span
  boundaries as constraints
  - read in classifier output
  - index synthetic binary rules in grammar
  - for each cell (other than the last one), if it spans a boundary
    then only loop over synthetic rules
- record number of times in inner CKY loop, as well as number of
  failed parses etc, using classifier from previous section at various
  thresholds and limited to different sentence lengths
- extract grammar from wsj02-21, evaluate on wsj22, include final test
  results for wsj23
- results

* Related Work

There are several strains of research related to adding constraints to
the CKY chart. TK demonstrated a classifier to decide whether any span
begins or ends at each word in a sentence. TK showed a classifier
which classifies each cell individually according to whether and how
many spans exist in it.

TK proposed a concept of 'hedge' parsing, where only spans below a
certain length are allowed, and show how this reduces the computation
done by CKY. In fact, their algorithm is quite similar to the modified
CKY algorithm used in this paper; however, their parser cannot create
spans of length larger than the threshold and thus doesn't follow the
original treebank annotation, while our approach is able to return the
original gold parse tree provided that the classifier does not output
a false positive.

** TODO add references
** TODO rewrite

* Conclusions

We have proposed an *independence* property of words in a sentence
derived from a parse tree, and shown how to use this property to
create top-down constraints which can be used to reduce the
computation done by the CKY algorithm. Then we demonstrated two
classifiers for identifying boundaries between independent words given
a sentence with only surface features, a linear classifier which is
fast but less accurate, and a classifier with a polynomial kernel
which is much more accurate but very slow. We then showed that a
commonly-used CFG parser can be made faster by using the output of
these classifiers to create top-down constraints at the cost of some
accuracy, which can be traded-off by varying the confidence threshold
of the classifier results.

Although the loss of accuracy when using the linear classifier is
currently too large to be practical, the performance of the kernel
classifier indicates that there is room for improvement by manually
adding conjunctive features to the linear classifier. Features based
on words as well as POS tags may also be beneficial. However, the
current approach has several weaknesses which should be addressed by
future research.

First, the top-down nature of the independence constraints does not
make a natural fit with the bottom-up CKY algorithm. In particular,
the binary nature of the rules in the grammar combined with the
bottom-up search means that the parser still ends up doing some
computation to create spans which violate the constraints, even though
it is prevented from completing such a span.

Second, the pipelined nature of the classifier means that it only has
access to POS tags and in particular is not able to make use of
information generated as the parser processes lower-level spans.

Third, the current classifier combines instances from different
syntactic structures into a single model. It is possible that training
multiple models on different types of sentences would result in a
better classifier.
