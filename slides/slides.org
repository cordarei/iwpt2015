#+TITLE: CKY Parsing With Independence Constraints
#+AUTHOR: Joseph Irwin and Yuji Matsumoto
#+DATE: IWPT2015
#+STARTUP: beamer
#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [presentation,bigger]
#+BEAMER_THEME: default
#+OPTIONS: h:2 toc:nil
#+COLUMNS: %45ITEM %10BEAMER_env(Env) %10BEAMER_act(Act) %4BEAMER_col(Col) %8BEAMER_opt(Opt)
#+PROPERTY: BEAMER_col_ALL 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.0 :ETC
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{fontspec}
#+LATEX_HEADER: \usepackage{xunicode}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{forest}
#+LATEX_HEADER: \usepackage[linesnumbered]{algorithm2e}
#+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}
#+LATEX_HEADER: \setromanfont{Source Sans Pro}
#+LATEX_HEADER: \newcommand{\deja}[1]{{\fontspec{DejaVu Sans}#1}}
#+LATEX_HEADER: \DeclareMathOperator*{\argmin}{arg\,min}
#+LATEX_HEADER: \DeclareMathOperator*{\argmax}{arg\,max}
#+LATEX_HEADER: \newcommand{\BigO}[1]{\ensuremath{\operatorname{O}\bigl(#1\bigr)}}
#+LATEX_HEADER: \newcommand{\Dag}{\ensuremath{^{\dagger}}}
#+LATEX_HEADER: \AtBeginSection[]{
#+LATEX_HEADER:   \begin{frame}
#+LATEX_HEADER:   \vfill
#+LATEX_HEADER:   \centering
#+LATEX_HEADER:   \begin{beamercolorbox}[sep=8pt,center,shadow=true,rounded=true]{title}
#+LATEX_HEADER:     \usebeamerfont{title}\insertsectionhead\par%
#+LATEX_HEADER:   \end{beamercolorbox}
#+LATEX_HEADER:   \vfill
#+LATEX_HEADER:   \end{frame}
#+LATEX_HEADER: }

# file:slides.pdf

* Motivation

** Motivation

- it would be elegant if we could divide (a significant proportion of) long sentences in a linguistically-motivated way

** Motivation

- current parsing techniques are generally \BigO{n^2}~\BigO{n^3}
  - possible speed gains by parsing smaller chunks at a time
  - $\left(\frac{n}{2}\right)^2+\left(\frac{n}{2}\right)^2 = \frac{n^2}{2} < n^2$

** Motivation

- bottom-up techniques:
  - chunking
  - clause boundary identification
  - constituent boundary identification (TK Roark et al)

** Motivation

- top-down techniques?


* Independence Constraints

** Independence Constraints

#+BEGIN_LaTeX
\resizebox{\textwidth}{!}{
\begin{forest}
[ROOT
  [S
    [NP-SBJ [DT [These]] [JJ [high-yielding]] [NNS [loans]]]
    [ADVP [IN [in]] [NN [effect]]]
    [VP [VBD [replaced]]
      [NP
        [NP [DT [some]] [JJ [low-yielding]] [NNS [assets]]]
        [PP [JJ [such]] [IN [as]]
          [NP
            [NP [JJ [inter-bank]] [NNS [loans]]]
            [{,} [{,}]]
            [SBAR
              [WHNP-1 [WDT [which]]]
              [S
                [VP [VBD [were]]
                  [VP [VBN [allowed]]
                    [S
                      [VP [TO [to]]
                        [VP [VB [decrease]]]]]]]]]]]]]
    [. [.]]]]
\end{forest}
}
#+END_LaTeX

** Independence Constraints

#+BEGIN_LaTeX
\resizebox{\textwidth}{!}{
\begin{forest}
[ROOT ,phantom
  [S ,phantom
    [NP-SBJ [DT [These]] [JJ [high-yielding]] [NNS [loans]]]
    [ADVP [IN [in]] [NN [effect]]]
    [VP [VBD [replaced]]
      [NP
        [NP [DT [some]] [JJ [low-yielding]] [NNS [assets]]]
        [PP [JJ [such]] [IN [as]]
          [NP
            [NP [JJ [inter-bank]] [NNS [loans]]]
            [{,} [{,}]]
            [SBAR
              [WHNP-1 [WDT [which]]]
              [S
                [VP [VBD [were]]
                  [VP [VBN [allowed]]
                    [S
                      [VP [TO [to]]
                        [VP [VB [decrease]]]]]]]]]]]]]
    [. [.]]]]
\end{forest}
}
#+END_LaTeX

** Independent Words

Given a sentence $s = w_1 w_2 \dots w_n$ and a context-free derivation (parse
tree) $t$ of $s$, words $w_i$ and $w_{i+1}$ are \textbf{independent} if every
node in $t$ that dominates both $w_i$ and $w_{i+1}$ also dominates $w_1$ and
$w_n$. Furthermore, if $w_i$ and $w_{i+1}$ are independent, then $\forall j,k$
s.t. $j \leq i$ and $k > i$, $w_j$ and $w_k$ are independent.

** Parsing: Ideal

#+BEGIN_LaTeX
\resizebox{\textwidth}{!}{
\begin{forest}
  top/.style={edge=red, for children={edge=red}, color=red}
[ROOT ,top
  [S ,top
    [NP-SBJ [DT [These]] [JJ [high-yielding]] [NNS [loans]]]
    [ADVP [IN [in]] [NN [effect]]]
    [VP [VBD [replaced]]
      [NP
        [NP [DT [some]] [JJ [low-yielding]] [NNS [assets]]]
        [PP [JJ [such]] [IN [as]]
          [NP
            [NP [JJ [inter-bank]] [NNS [loans]]]
            [{,} [{,}]]
            [SBAR
              [WHNP-1 [WDT [which]]]
              [S
                [VP [VBD [were]]
                  [VP [VBN [allowed]]
                    [S
                      [VP [TO [to]]
                        [VP [VB [decrease]]]]]]]]]]]]]
    [. [.]]]]
\end{forest}
}
#+END_LaTeX

** Parsing: Reality

- independent boundaries must be identified automatically
- classifier will make errors
  - compensate for false negative by doing extra computation
- grammar binarization
  - the span of an incomplete constituent differs from the span of its eventual complete constituent (possibly arbitrarily)

** Parsing: Reality

\centering

file:../paper/chart-constraints.pdf

CKY Chart

* Classifying Independent Span Boundaries

** Classifying Independent Span Boundaries

- binary classifier: is $k$ an independent span boundary?
- instance for every $1 \le k < n$
- pointwise model
- POS features
  - global features
  - heuristic transformations of POS sequence (POS /level/ )

** Features

#+BEGIN_LaTeX
  \centering
\begin{tabular}{ll}
 \multicolumn{2}{c}{\bf Local Features} \\
 \hline
 $t_{k-1}$                 & $t_{k}$                 \\
 $t_{k-2},t_{k-1}$         & $t_{k},t_{k+1}$         \\
 $t_{k-3},t_{k-2},t_{k-1}$ & $t_{k},t_{k+1},t_{k+2}$ \\
\end{tabular}

\begin{tabular}{ll}
 \multicolumn{2}{c}{\bf Global Features} \\
 \hline
  $t^l_{i}$                     & $1 \le i < k - 1$ \\
  $t^l_{i},t^l_{i+1}$           & $1 \le i < k - 2$ \\
  $t^l_{i},t^l_{i+1},t^l_{i+2}$ & $1 \le i < k - 3$ \\
  $t^l_{i}$                     & $k \le i < n - 1$ \\
  $t^l_{i},t^l_{i+1}$           & $k \le i < n - 2$ \\
  $t^l_{i},t^l_{i+1},t^l_{i+2}$ & $k \le i < n - 3$ \\
\end{tabular}

#+END_LaTeX

** POS Levels

#+BEGIN_LaTeX
\centering
\scriptsize

\begin{tabular}{llllllll}
Lvl0 & Lvl1 & Lvl2 & Lvl3 & Lvl0 & Lvl1 & Lvl2 & Lvl3\\
\hline
NN & N & N & N & CD & X & X & \#\\
NNP & N & N & N & -LRB- & X & X & B\\
NNPS & N & N & N & -RRB- & X & X & B\\
NNS & N & N & N & DT & X & X & D\\
PRP & N & N & N & PDT & X & X & D\\
VB & V & V & V & PRP\$ & X & X & D\\
VBD & V & V & V & WP\$ & X & X & D\\
VBG & V & V & V & JJ & X & X & J\\
VBN & V & V & V & JJR & X & X & J\\
VBP & V & V & V & JJS & X & X & J\\
VBZ & V & V & V & -RQ- & X & X & Q\\
, & X & , & , & -LQ- & X & X & Q\\
. & X & . & . & RB & X & X & R\\
: & X & : & : & RBR & X & X & R\\
CC & X & C & C & RBS & X & X & R\\
IN & X & I & I & EX & X & X & X\\
RP & X & I & I & FW & X & X & X\\
TO & X & T & T & LS & X & X & X\\
WDT & X & W & W & MD & X & X & X\\
WP & X & W & W & POS & X & X & X\\
WRB & X & W & W & SYM & X & X & X\\
\# & X & X & \# & UH & X & X & X\\
\$ & X & X & \# &  &  &  & \\
\end{tabular}

#+END_LaTeX

** POS Levels

- Level 0: Original tags
- Level 1: Noun+Verb+Other tags (clause nuclei)
- Level 2: Noun+Verb+Punct+Conj+Prep+Other (clause boundaries)
- Level 3: Merge related tags (reduce features, smoothing)

** POS Levels: Example

[example]

** Experimental Setup

- =opal= online classifier
- trained on WSJ sections 01-21
- development on WSJ section 22

** Results

[graph feature conf on prec-rec plain]

** Results


* Parsing With Independence Constraints
  
** CKY Algorithm

\small
#+BEGIN_LaTeX
\begin{algorithm}[H]
  % \caption{The CKY algorithm. $T_{i,j}$ is the cell corresponding to words $w_i \dots w_{j-1}$.\label{alg:cky}}
  \DontPrintSemicolon
  \For {$1 \le i \le n$}{
    $T_{i,i+1} \gets \{A|A\rightarrow a \in G \wedge w_i = a\}$
  }
  \For {$2 \le j \le n$}{
    \For {$1 \le i \le n-j+1$}{
      \For {$i < k < i+j$}{
        $T_{i,i+j} \gets \{A|A\rightarrow BC \in G \wedge B \in T_{i,k} \wedge C \in T_{k,i+j} \}$\;
      }
    }
  }
\end{algorithm}

#+END_LaTeX

** Revised CKY Algorithm

\small
#+BEGIN_LaTeX
\begin{algorithm}[H]
  % \caption{The CKY algorithm. $T_{i,j}$ is the cell corresponding to words $w_i \dots w_{j-1}$.\label{alg:cky2}}
  \DontPrintSemicolon
  \For {$1 \le i \le n$}{
    $T_{i,i+1} \gets \{A|A\rightarrow a \in G \wedge w_i = a\}$
  }
  \For {$2 \le j \le n$}{
    \For {$1 \le i \le n-j+1$}{
      \For {$i < k < i+j$}{
        \If {$w_i$ and $w_{i+j-1}$ independent}{
          $T_{i,i+j} \gets \{A|A\rightarrow BC \in G\setminus{}G_{comp} \wedge B \in T_{i,k} \wedge C \in T_{k,i+j} \}$\;
        } \Else {
          $T_{i,i+j} \gets \{A|A\rightarrow BC \in G \wedge B \in T_{i,k} \wedge C \in T_{k,i+j} \}$\;
        }
      }
    }
  }
\end{algorithm}

#+END_LaTeX

** Experimental Setup

- implemented constraints into Stanford Parser
  - ExhaustiveCYKParser
- unlexicalized grammar extracted using Stanford Parser from WSJ 01-21
- tested on WSJ section 23

** Results of parsing WSJ section 23

#+BEGIN_LaTeX
  \centering
\begin{tabular}{lclll}
{\bf Parser}           & {\bf Constraints\hspace{-.5em}} & {\bf Time} (s) & {\bf Speedup}      & {\bf F$_{\text{1}}$} \\
\hline
baseline               &                   & 1538           &                    & 85.54                \\
\multirow{3}{1.75cm}{+constraints} & \multirow{2}{*}{linear}            & 1106           & 1.39\texttimes{}   & 83.55 (-1.99)        \\
                       &                   & {\small{}+100\Dag}         & {\small{}(1.28\texttimes{})} &                      \\
                       & poly              & 1040           & 1.48\texttimes{}   & 84.57 (-0.97)        \\
\end{tabular}
#+END_LaTeX

\Dag Time taken by Python script

# 1.28 speedup == time reduced by 22%
# 1.39 speedup == 28%

* Wrap-up

** Summary

- proposed constraints for parsing based on independent span boundaries
- demonstrated a classifier for recognizing independent span boundaries
- integrated constraints into CKY parser

# for a speedup of let's say 30% at cost of 2 F1 score

** Future Work

- improve  model
- pipeline vs joint architecture
- apply constraints to other parser technologies

* Questions
